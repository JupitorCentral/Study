

## Primitive Type, reference Type 이 저장되는 공간은 어디인가 ?

자바에는 이미 정의된 primitive 타입이 있다.  로우케이스.
얘네들의 wrapper 클래스는 capital letter로 시작, 그리고 API 에 정의된 Class 이므로 Method 가 있음.

### Primitive on the stack and heap

==Primitive Type 은 스택 또는 힙에 저장 될 수 있다.== 

local variable, paramter 일 경우 stack 에 저장됨.
member of class 의 경우 (instance variable), instance 가 생성되면서 'heap' 에 저장됨.

-> 그럼 class variable 은 ?
-> metaspace (heap 과 별도로 분리된 공간.)

### Storing object on the heap

#### Reference

reference 는 객체를 가리키고, 우리가 그 객체에 접근할 수 있게 해줌. (여기서 reference 라는 특정 개념이 나오는 듯 하다)
우리가 object instance member (instance variable) 에 접근할때, 우리는 'reference' 를 사용한다.

만약 우리가 static variable 에 접근하면, 우리는 class name 을 사용 (reference 를 사용하지 않는다는 것으로 보임)

reference 는 스택과 힙에 저장.
스택 -> reference 가 local variable 일 경우
heap -> instance variable (variable inside a class) 일 경우 heap

(heap 은 동적 생산이 일때 사용되는 공간이 맞는 것 같다.)

##### reference to others 

abstract class 에 대해 reference 를 가질 수 있음 (abstract class 가 아니라. 근데 애초에 abstract class 가 객체를 가질 수 없잖아)
interface 도 마찬가지.


#### Objects

모든 객체는 'heap' 에 저장된다.
객체를 이해하려면 OOP 의 근간이 되는 이론을 알아야 함.
class 는 집의 설계도. 집이 만들어지면, 그제서야 문을 열고 창문을 닫을 수 있음.

객체가 생성되면, in-memory representation of the class 를 갖게되는 것.
reference 를 이용해서, 우린 instance members 에 접근 할 수 있음 (dot notation syntax)



#### Difference between references and objects

![[Screenshot 2025-09-15 at 12.33.50 AM.png|700]]

```java
class Person {  
    private String name;  
    private int age;  
  
    Person (String name, int age) {  
        this.name = name;  
        this.age = age;  
    }  
  
    @Override  
    public String toString() {  
        String decoratedName = "My Name is " + name + " and I am " + age + " years old";  
        return decoratedName;  
    }  
  
    public void setName (String name) {  
        this.name = name;  
    }  
}  
  
public static void main(String[] args) throws Exception {  
    Main mainObj = new Main();  
    int x = 0;  
    Person joeBloggs = mainObj.new Person ("Joe Bloggs", 23);  
    System.out.println(x);  
    System.out.println(joeBloggs.toString());  
    changeName(joeBloggs, "John");
}  
  
public static void changeName (Person person, String newName) {  
    person.setName(newName);  
}
```

이 코드를 실행하는 method 에 대해서, 
첫번째 main () method 가 있고, main () 함수를 위한 frame 을 스택에 쌓게 됨.
그리고 그 프레임안에 main method 의 local variable, parameter 가 생성됨.
이때 new 로 생성된 Person 객체에 대한 reference joeBloggs 도 main 안에 생기게 됨 (local variable 로써)

String Pool 도 Heap 공간에 저장되고, 어떤 새로운 내용을 가진 String Object 가 생기면 거기에 저장해두었다가
나중에 객체를 재사용함. 
그러니까.
```java
String s1 = "My name is Joe Bloggs";        ->> "My name is Joe Bloggs" 라는 String 객체가 String pool 에 생성
String s2 = "My name is Joe Bloggs";         --->> String Pool 에 있던 객체 그대로 씀
```

그러니까 s1, s2 는 reference 이고, "My name is Joe Bloggs" 얘는 "My name is Joe Bloggs" 라는 값을 가지는 String 객체라는 것.
("My name is Joe Bloggs" 같은건 String literal 이라고 한다.)

println method 가 콜 될때, 그를 위한 Frame 이 stack  에 push 됨.
println 실행이 끝나기 전에, joeBloggs.toString() 이 반드시 호출되어야 한다 (-> frame 이 생성됨)

그러니까, 먼저 System.out.println(x);  에 의해서 println 에 대한 frame 생성
그 다음줄에서 toString method 를 위한 프레임 생성 이런 흐름인듯.

이게 헷갈릴 수 있는게, 참 웃긴게 
joeBloggs.toString() 이 실행될때 toString() 에 대한 frame 이 stack 에 쌓이면서
toString() 의 local variable 인 reference decoratedName 이 생성되고, 
String literal 은 Heap 에 생성된다는 것이다.

타이밍은 내가 생각했을땐, toString 에 대한 method 를 위한 frame 이 생성될때 그 메서드 안에 쓰이는 
String literal 들이 String pool 에 생성될 듯.


그래서 class 에 대한 객체가 생성될때 reference 는 객체를 참조하고,
이 reference 가 함수의 parameter 로 넘어갈때 결국 이 reference 자체가 넘어가는 것이 아니라
reference 의 '값' 이 넘어가기 때문에 그리고 이 참조값은 이전에 생성했던 
new Person ("Joe Bloggs", 23);  -> 이 객체이기 때문에
changeName 에서 person 이 new Person ("Joe Bloggs", 23); 객체를 가리키게 되는 것.

결국 참조에 대한 call by value 가 되면서 call by reference 처럼 보이게 되는 것 -> 이거 사람들 사이에 의견이 분분한 것으로 알고 있다.
근데 메모리 관점에서 보면, call by value 가 일단 맞는 듯 하다. 이런 관점에서 보면.

그러니까, changeName executed -> frame for changeName created to stack 
-> person reference created inside that frame
-> call by value, so value related to new Person instance is copied to person local variable (reference) inside that frame.

그러니까 결국 포인터 개념이 맞네. cpp 처럼.

이 이후는 좀 내용이 너무 많아서 pass 하자.

-> 결국 reference 를 잘 다루어야 한다는 얘기.




## Static Field 는 클래스가 memory 에 loading 될때 저장되는데, 이에 대한 더 정확한 설명


at Java Memory Management by van Putten, Maaike (2022), Chapter 5
### Zooming in on the Metaspace


#### Why Metaspace, not Perm ?
-> Java 8 이전에는, 적어도 내가 원래 학습했던 Heap 의 구조는
Young (Eden - s0 - s1 + virtual) + Old (Old/Tenured + virtual) + Perm (virtual) 이었다.
[[Spaces/Study/F-Lab/2주차 공부/250912.md#^8cf8de|참조]]

그런데 Perm Generation 이 Fixed size 다 보니, OutOfMemoryError (OOME) 을 일으키는 문제가 있었다.
자바는 이 Permgen 의 사이즈를 수동으로 변경할 수 있게 허락하지만, Permgen 이 자동적으로 올라가게는 설계가 되어있지 않았음.
그러니까 tune 하기 어려움. 또한, GC 가 Perm 공간을 처리하기에는 충분히 효율적이지 못함.

따라서, Java 8 로부터 이 클래스 메타데이터, 상수등을 저장하는 공간이 Heap 바깥으로 옮겨졌으며, 
한도가 정해지지 않는한 필요에 따라 JVM 이 시스템 메모리 한계치까지 확장가능하도록 설계가 바뀜.
(application requirements 에 따라 유동적으로 크기를 조절함)

원래 PermGen 에는
class metadata, interned strings, class's static variables 가 들어가 있었다.
Java 8 부턴, 클래스 메타데이터는 Metaspace 에, 
interned strings 와 class / static variables 은 heap 에 저장되게 됨.

#### What constitutes Metaspace

- 클래스 파일
- 클래스의 구조 (?) 와 메서드
- 상수
- 어노테이션
- Optimizations (?)


-> 좀 더 공부가 필요함.


## List\<Integer> arr <- 이 arr 에 null 을 add 할 수 있을까 ? 


```java
public static void main(String[] args) throws Exception {  
  
    ArrayList<Integer> arr = new ArrayList<>();  
  
    arr.add( 3 );  
    arr.add( 5 );  
    arr.add(null);  
      
    for (Integer x : arr) {  
        print(x);  
    }  
}
```

분명, 위 내용으로 봤을때, ArrayList 에 add 가 추가되는것은, reference 가 추가되는 것일 것이다.
그렇지 않으면 null 이 될 수가 없다.

근데 primitive type 은 어떨지 궁금하다.
내부적으로 3 의 값을 가지는 Integer object 를 생성하고 그를 가리키는 reference 를 추가하는게 아닐지 싶다.
그러니까 arr.add ( 3 ) 은 arr.add ( Integer.valueOf( 3 ) ) 이 아닐지?

(intever.valueOf 도 코드를 보면 새로 생성해서 리턴해준다. Effective Java 에 나온 이름이 있는 Constructor Factory 패턴.)



## 캐싱 사용패턴 3가지

by Software Architect's Handbook - Joseph Ingeno
에서 읽어보니 4가지더라.

- system-of-record
-> Datasource 를 말함. 원본 데이터가 실제로 저장되는 곳 -> DBMS

- [ ] envict -> 퇴거시키다, 내쫒다 #EnglishWord  ➕ 2025-09-15 


caching 이 먼저 뭔지 알아야 할 것 같은데...

### What is Caching ?

caching 은 cross-cutting concers  중의 하나이다.

### What is cross-cutting concerns ?

소프트웨어 시스템에서, 'concern' 이라 함은 어플리케이션이 제공하는 로직, 기능의 묶음.
concern 은 요구사항을 reflect 한다.
-> 그러니까 만들고자 하는 소프트웨어가 어떤 기능, 로직을 제공할 것인가를 software architect 가 찾아야 하는데,
이 어떤 기능, 로직을 concerns 라고 표현함.

concerns 에는 2가지가 있다.

#### Core concern

-> 시스템이 만들어져야 하는 이유가 되는, 근본적인 기능을 의미.
예를들면, 직원의 연봉과 보너스를 계산하는 기능은 인사관리체계 시스템에 필수적인, 즉 core concern 이다.

#### Cross-cutting concern

-> 여기서 Cross-cutting 이라 함은, '가로지르며 횡단하는' 을 의미.

- [ ] crosscut #EnglishWord ➕ 2025-09-15 
     1. cut (wood or stone) across its main grain or axis.
		"they crosscut the timber before its removal"
		
		main grain or axis -> 나무의 주요 섬유 방향, 또는 주축 (주가 되는 축)
		보통 나무 섬유는 주로 세로 방향으로 형성된다 (단면을 딱 자르면)
		
		그러니까 나무를 가로로 자르는 행위임.
	1. alternate (one sequence) with another when editing a movie.
		"the opening sequence crosscuts the reluctant confessions of a young wife with the hippy's arrival"
		
		오프닝 시퀀스 -> 영화 시작부분
		crosscut 한다는 의미는, 두 개 이상의 장면을 번갈아 가면서 편집하는 기법
		-> 그러니까, 젊은 아내의 '주저하는' 고백과 히피의 도착을 교차 편집한다, 가 됨.


![[Screenshot 2025-09-15 at 11.12.45 PM.png|400]]

그래서 cross-cutting concern 이라 함은, 메인 기능은 아니지만, 어플리케이션이 기대는 것 또는 다른
'concerns' 에 영향을 미치는 것이라 볼 수 있으며, 예를들어 Security, logging 등이 있겠다.

즉! 바로 caching 은 cross-cutting concerns 중에 하나라는 것이다!!!

-->> caching 에 대한 정의를 너무 소프트웨어 공학 적으로 접근한 것이 아닌지. 
단순히 캐싱은 자주 사용되는 데이터들을 빠른 메모리에 올려두어 request 에 대한 성능을 증가시키는 것
이라고 봐도 무방한데 말이다.


### Cache usage patterns

크게 2가지 로 나뉠 수 있는데, 
어플리케이션은 DB에 읽고 저장하는 것을 포함해서 캐시 데이터 그 자체를 유지할수도 있고, (이게 cache aside)
cache 를 system of record 로 취급해서, 캐시 시스템이 DB 에 reading/writing 을 다루게 할 수도 있다.
-> 여기서 2번째 줄에서 말하는 db 는 그럼, cache storage = system of record 인가 ?

->뭔소리야 이게 ?
그러니까 캐시 시스템한테 맡기던가 아님 어플리케이션이 스스로 관리하거나 이건가 ?

#### Cache-aside pattern

(캐시 배제패턴.)

이 패턴은, 캐시에 데이터를 유지하는 것에 대한 책임이 어플리케이션에 있음.
The cache is kept 'aside' , (캐시가 별도로 관리된다는 의미임. aside 는 별도로, 분리되어 라는 의미로 쓰임)
그리고 캐시는 DB 와 직접 소통하지 않음.

요청하는 값이 cache 에 있으면, system-of-record 가 bypass 되고, cache 에 있는 값이 return 됨.
값이 cache 에 존재하지 않으면, system-of-recrod 에서 값을 가져와서, 캐시에 저장하고, request에 맞춰 반환한다.

db 에 저장되어있는 데이터가 변경되면, 어플리케이션이 반드시 해당하는 cache 데이터를 invalidate 하면서
cache 가 system of record 와 동일함을 make sure 해야 한다.

그런데 항상 캐시된 데이터가 db 와 일치할 것으로 예상하는 것은 비현실적이다.
그래서 나온 해결방법이 아래 방식.

##### pros 

쉽고 직관적인 방식.

'Allows fine-grained control over cache operations.'
캐싱 동작에 대해서 세밀하게 직접 짤 수 있음

##### cons
-> 어플리케이션이 관리하므로 캐싱 관리에 대한 알고리즘을 어플리케이션 코드에 집어넣어야 함.
-> 관리 잘 못하면 outdated data (유효한기간이 지난.) 발생

#### Read-through pattern

이 패턴에선,  cache = system of record 이고, 
진짜 system of record (database) 에서 값을 가져올 수 있는 어떤 컴포넌트가 있다.
그리고 'application' 이 값을 요청하면, cache system 이 cache 로부터 값을 가져올려고 하고,
없으면 (컴포넌트를 통하겠지?) database 에서 값을 가져와서, cache 에 저장하고, 필요한 값을 어플리케이션에게 넘겨줌.
-> db 에서 읽기만하고 쓰진 않는다.

그러니까 cache system의 존재 유무가 큰 것 같다.


#### Write-through pattern

이 패턴에선 cache system 의 컴포넌트가 database에 값을 쓸 수가 있다.
그래서 어플리케이션은 이 cache system 을 system of record 라고 취급하고, 
어플리케이션이 요청하면 cache system 은 db 에 데이터를 write 하고 
cache 를 업데이트 한다. 



#### pros & cons of Read/Write through pattern
##### pros 
cache 관련 코드를 어플리케이션에서 구현하지 않아도 되니 어플리케이션 코드가 간단해짐
캐시와 db 간의 일관성을 보장

##### cons
cache 에 데이터가 없어 db 로부터 값을 불러와야 하는일의 빈도에 따라
latency 가 심해질 수 있음



#### Write-behind pattern

이 패턴은 때때로 write-though pattern 대신에 쓰인다.
write-through, write-behind 모두 cache system 에 있는 cache 를 system of record 로써 사용하지만,
db 에 값을 쓰는 타이밍이 살짝 다르다.

write-through pattern 에서는 쓰레드가, cache system 이 db 에 값을 쓰는걸 완료하기 까지 기다린다.
write-behind 패턴은 이 동작 (writing to db) 을 queue 에 집어넣는다.

이러한 패턴의 장점은, thread 가 다른 일을 하기 위해 빨리 이동할 수 있다는 것.
대신, cache 와 db 간의 inconsistent 가 발생하는 짧은 시간이 있다는게 단점.

-> 그러니까 캐싱에 먼저 쓰고 db 에 쓰는걸 좀 미룬다. 비동기적으로 처리.

#### Write back pattern

얘는 cache 와 db 에 쓰는걸 미룬다. 특정한 trigger 또는 event 가 발생할때까지 말이다.


#### pros & cons of Write behind/back pattern
##### pros 
cache - db 간의 delaying updates 에 따른 application performance 의 저하를 줄일 수 있음
쓰기 동작을 batching & optimizing 할 수 있음

(batching 할 수 있다는게 뭐지 ?)

- Time shifting - moving writes to a specific time or time interval. For example, writes could be batched up and written overnight, or at 5 minutes past the hour, to avoid periods of peak contention.

-> 즉 그때그떄 마다 쓰는게 아니라 여러개의 작업을 모아서 한꺼번에 수행
-> Write 를 한꺼번에 함으로써 많은 write 작업으로 인한 시간을 최소화.

##### cons

write behind/back 은 기본적으로 Deffered Gratification 임.
즉, 바로바로 수행하는게 아니라 수행 자체를 미루는 것.

그러다보니 그 미루는 기간동안에 cache - db 간에 data inconsistency 가 발생할 수 있음.
그리고 비동기화다보니까 더 복잡한 메커니즘이 운용하는데에 필요함.

