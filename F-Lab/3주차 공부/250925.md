

## 캐싱 스탬피드란 ?

- Stampede

Noun : a sudden panicked rush of a number of horses, cattle, or other animals.
"the herd was fleeing back to the high land in a wild stampede"

패닉에 빠진 말, 소 그 외의 갑작스러운 습격.
-> 갑자기 request 가 물밑듯이 들어와 

Verb : (of horses, cattle, or other animals) rush wildly in a sudden mass panic.
말 소 따위의 동물이 갑작스런 엄청난 패닉에 빠져 야만적으로 급습하다

---

캐쉬 스탬피드란, cascading failure 의 일종으로써,
캐싱 매커니즘이 탑재된, 엄청난 크기의 병렬 컴퓨팅 시스템이 엄청난 부하에 휩싸일때 일어나는 
cascading Failure 이다.

페이지 캐싱은 똑같은 페이징을 요구하는 사용자에게 캐싱된 데이터를 이용함으로써
서버부하를 감소시키는데 사용된다.
만약 cache miss 가 나더라도, 적은 부하일 경우 서버에서는 해당 페이지를 다시 렌더링하여 반환해주면 된다.

하지만, 만료된 캐시에 대해서 짧은 시간에 수많은 request 들이 몰릴 경우
수많은 여러개의 서버 프로세스는 동일한 요청에 대해서 순식간에 페이지를 렌더링 하게 되는데,
이때 프로세스들은 서로가 동일한 페이지를 렌더링하는 사실을 모르고,
이로 인해 분산 컴퓨팅에서의 공유 자원이 순식간에 고갈나면서 시스템의 congestion collapse 를 일으키게 되는 것이다.

```java
## Cache Stampede의 연쇄 반응 과정

1. **초기 실패**: 캐시가 만료됨
    
2. **부하 재분배**: 수십~수백 개의 프로세스가 동시에 같은 데이터를 재계산하기 시작[](https://en.wikipedia.org/wiki/Cache_stampede)
    
3. **과부하 발생**: 데이터베이스나 백엔드 시스템에 평소보다 훨씬 많은 요청이 몰림
    
4. **2차 실패**: 백엔드 시스템이 과부하로 응답 속도 저하 또는 실패
    
5. **실패 전파**: 더 많은 프로세스가 대기하며 시스템 자원 고갈, 전체 시스템 마비로 확산[](https://en.wikipedia.org/wiki/Cache_stampede)
```


### Cascading Failure

Cascading Failure (연쇄 실패) 란, 연결된 파츠들로 이루어진 시스템에서, 
하나 또는 소수의 파트들의 failure 연결된 다른 파츠들의 failure 로 이어지면서,
failure 들이 악순환을 일으켜 점점 그 오류가 커지는 것을 말한다.

그러니까 퍼져나가는 오류. 

cascading failure 는 시스템 중 한 부분이 고장나면 일어날 수 있으며,
이를 막기위해 하나의 부분이 망가지면 이를 대체할 다른 시스템의 일부분을 만들어 두어야 한다.



### Network Congestion & Congestive collapse

Network Congestion 이란, 네트워크의 링크 또는 노드들이 그것들이 가진 용량보다
더 큰 부하를 짊어지거나 처리하게 되는 경우 일어나는 서비스 제공 품질의 저하이다.

그로 인해 발생하는 문제들은 Queuing Delay ( 처리를 위해 queue 에서 기다리는 작업들의 기다림이 지연되는 현상 ),
packet loss (패킷이 목표까지 도달하지 못하고 사라지는 경우), 또는 새로운 커넥션을 막는 현상 등이 있다.


Congestive (Congenstion) collapse 는, 네트워크 혼잡이 원활한 커뮤니케이션을 방해하거나 제한하는 상황을 말한다.



### 근데, 캐싱 시스템이 아니어도 이런 일은 일어나지 않나? 같은 부하라면 ?

캐싱시스템의 히트율이 올라갈 수록 시간당 요청개수가 늘어난다.
예를들어 원래 1초당 요청 10개에 대해 처리하기 위해서 서버 10대가 필요하다면,
캐시 히트율이 50%일때, 요청 5개만 처리하면 되므로 서버가 5대만 필요하게 된다.

그런데 캐시 스탬피드, 즉 캐싱 failure 가 동시에 일어나면, 그러니까 caching miss 가 일어난
필요한 데이터에 대해서 동시에 10개의 요청이 일어나면
서버 5대가 동시에 그 작업을 처리하게 되고 (똑같은 렌더링을 불필요하게), 
그러면 캐싱이 무력화되어 동시 10개를 무리하게 처리하려다가 과부하가 발생하게 되는 것.

이러면서 Cascading Failure 가 일어나게 되는 것 같다.



### 해결방법

결국 한 서버가 같은 처리를 하는 것을 다른 서버가 모르기때문에,
동시성 제어가 필요한 부분인 것.

또는 cache eviction 알고리즘을 최적화한다.
(cache failure 비율을 낮춘다)


## 캐시 expired 에 대한 고민 - Caching Eviction Strategy


그렇다면 캐쉬 스탬피드를 해결하기 위해서는, cache eviction 을 어떻게 할 것인가가 중요해진다.
가능한한 cache failure 가 안 일어나야 하니깐.

###  **Least recently used (LRU)** 

Per this strategy, the cache item that's been accessed the least recently is removed from the cache when the cache reaches its maximum capacity. Note that this approach assumes entries that have been the least used will not be used again in the near future.

cache capacity 가 꽉 찼을때, 가장 오래전에 조회된 캐시데이터부터 삭제.
이 접근법은 가장 오래전에 접근된 캐시가 가까운 미래에 다시 사용되지 않을 것을 전제로 함.

###  **Most recently used (MRU)** 

Cache entries that have been accessed most recently are evicted when the cache is full. As per this assumption, entries that have been used more recently are less likely to be used again in the near future.

위와 반대로, 캐시 용량이 가득 차면 가장 많이 사용된 캐시가 폐기되는 형태.
즉 최근에 가장 많이 사용된 데이터가 가까운 미래에는 덜 사용될 것이라는 가정을 가짐.


###  **Least frequently used (LFU)**

This policy stipulates that when the maximum cache capacity is reached, the cache element accessed least frequently within a certain period is removed. It asserts that entries used less often are unlikely to be accessed in the future.

- [ ] stipulate ➕ 2025-09-25 #EnglishWord 
      demand or specify (a requirement), typically as part of a bargain or agreement.
      "he stipulated certain conditions before their marriage"

이 방법은 캐시 용량이 가득 차면, 일정 주기동안 가장 적게 접근된 캐시가 지워질 것을 요구한다. (demand)
이는 가장 적게 접근된 데이터가, 가까운 미래에 덜 접근될 것이라는 주장을 하는 것.


###  **Time-to-live (TTL)**

This policy defines the duration for which a cache entry should remain valid in the cache. A cache entry is removed when its time-to-live (TTL) expires. You can employ this method when the data in your application has a short duration, like session data or refreshed data. It can prove beneficial in scenarios where your application relies on data that has a short lifespan such as session data or data that undergoes frequent updates.

이 정책은 어떠한 캐시 데이터가 특정 시간동안 유효하다고 남아있을 것을 정의한다. 
캐시 엔트리 (데이터) 는 이 엔트리의 TTL 이 만료되면 삭제된다. 

이 방법은 세션 데이터나 갱신된 데이터와 같이 어플리케이션에서의 데이터가 짧은 유지기간을 가질때 사용할 수 있음.
의존하는 우리의 어플리케이션이 세션데이터나 짧은 업데이트 주기를 가지는, 생명주기가 짧은 데이터에 의존하는 시나리오에서
이 방법이 효과적임을 증명할 수 있음.

그러니까 데이터마다 만료시간이 있음.


## 논문 - Optimal Probabilistic Cache Stampede Prevention
by Andrea Vattani, Flavio Chierichetti, Keegan Lowenstein

최적의 확률적 캐쉬 스탬피드 방지 (전략)

```java
**"A natural countermeasure to this issue"**  
→ 이 문제에 대한 자연스러운 대응책은

**"is to let the processes that perform such requests"**  
→ 그러한 요청을 수행하는 프로세스들로 하여금

**"to randomly ask for a regeneration before the expiration time of the item"**  
→ 아이템의 만료 시간 이전에 무작위로 재생성을 요청하도록 하는 것이다
```

무작위로 재생성을 요청한다 ...?


본인들의 알고리즘은 이론적으로 최적의 방법이고, 다른 방법들보다 실제 세계의 어플리케이션에서 더 나은 퍼포먼스를 보여준다...


#### TTL 만료 방법을 쓰는 캐쉬 안에 있는 데이터를 불러오는 일반적인 패턴.

```java
function Fetch(key, ttl)
	value ← CacheRead(key)
	if !value then
		value ← RecomputeValue()
		CacheWrite(key, value, ttl)
	end
	return value
end
```

> [!info] 캐싱에서 보통 Entry 는 하나의 완성된 캐시 데이터 단위를 말함 



Fetch (key, ttl) 이렇게 되어있는데,
왜 ttl 이 필요하냐면 해당 캐쉬 데이터가 없을 경우 
CacheWrite 를 통해서 어떤 ttl 값을 가지고 캐시 데이터를 설정할 것인지 정해야 되기 때문이다.

그러니까 Fetch : 캐시 데이터를 조회한다
-> CacheRead : 캐시 데이터를 불러온다
-> 캐시데이터가 존재하지 않으면 -> 캐시 데이터를 '계산' 한다
-> cache를 설정한다.

이때 '계산' 하는게 비용이 많이 든다.

```java
// 사용자 프로필 "계산"
RecomputeValue() {
    user = database.findUser(userId);           // DB 쿼리
    avatar = imageService.getAvatar(user);      // 외부 서비스
    permissions = authService.getPermissions(); // 권한 조회
    return buildUserProfile(user, avatar, permissions);
}
```
-> 여기서 '계산' 한다는 의미는 캐싱할 데이터를 완성하기 위한 일련의 작업들을 말한다.


논문에서는 캐시 스탬피드에 대한 완화 정책으로 3가지를 말한다.
#### 캐시 스탬피드 완화 방법 3가지 

##### 1. 외부 재계산
request 자체가 캐싱을 다시 생성하는 대신 (요청을 처리하는 쓰레드가 주체인것을 뜻하는 듯함),
주기적으로 캐시데이터를 재 생산하는 백그라운드 프로세스를 두는 것.
이 방법은 캐시 스탬피드를 모두 해결하지만 (왜 ?), 종종 기각되는데 그 이유는
별도의 프로세스를 두는 것에 대한 비용때문임.
(데몬 프로세스, 모니터링도 두어야 하고, 코드 분리와 반복 등)

그래서 이는 점점 어려워지는데 (This becomes even more daunting), 
왜냐하면 백그라운드 프로세스가 필요하지 않은 데이터를 주기적으로 생성할때마다
어떤 데이터를 주기적으로 생성할지 각 데이터마다 '추적' 이 필요하기 때문.
(결국 필요하지 않은 데이터를 생성하고 이는 컴퓨팅 자원의 낭비가 됨)

- [ ] daunting ➕ 2025-09-25 #EnglishWord 
      seeming difficult to deal with in anticipation; intimidating.
	  "a daunting task"

- [ ] in anticipation ➕ 2025-09-25 #EnglishWord 
	with the probability or expectation of something happening.
	"they manned the telephones in anticipation of a flood of calls"  


#####  2. 잠금 - Locking
cache miss 가 날때, request 는 해당 cache key 에 대해서 lock 을 얻으려고 하고, 
시스템은 request 가 해당 lock 을 획득할때만 캐쉬 데이터를 재생성함.

어떤 lock mechanism 이 선택됨에 따라 캐시 스탬피드를 완화시키거나 심지어는 아예 없앨 수 있음.

이러한 방법의 한가지 문제점으로는
stempete 에 속하게 됬을 모든 요청들이 (사용자에게) 돌아갈 목적으로 가져갈 캐시 아이템이 없다는 것.
(all requests ... have no cache item to return.)

뭐 이에 대한 적은 선택지들이 있음.

1. 클라이언트가 캐시 아이템의 부재에 대해서 적절하게 처리하게 하거나
   
2. 락을 획득하지 못한 요청들이 아이템이 재생성될때까지 기다리게 만들거나
	(have the requests not acquiring the lock wait for the item to be regenerated)

3. 또는 새로운 값이 생성될 동안에 캐쉬에 stale item 을 둔다거나. 
	(stale item -> 이전 버전의 데이터)


###### cache item states in the Stale Cache pattern

- **Fresh**: The item has not yet reached its Stale (or best-before) date. 
  The item is not stale yet and can be used without any additional actions.
  
- **Stale**: The item has passed its Stale date, but not its Expiry date. 
  The item is stale, but can still be used safely. A fresh item should be requested from the source and the cache updated with the fresh item.
  
- **Expired**: The item has passed both its Stale and Expiry date. The item is expired and cannot be used safely. The expired item should be removed from the cache and discarded. A fresh item should be requested from the source and the cache updated with the fresh item.

	Facebook, Netflix 등의 서비스에서 이런 전략을 쓴다는데 ...?


이러한 방법들은, 추가적인 locking mechanism 을 구현해야 하는 비용이 있고
ttl 을 이 locking mechanism 에 맞게 조정해야 한다.

가장 무엇보다 이 방법은 fault-tolerant 즉 오류에 대한 저항성이 없음.
락을 획득한 request 가 item 을 re-generating 하는 도중에 오류가 나면, 
어떠한 아이템 - stale item 까지도 request 는 반환받지 못할 것이다.
-> lock 이 만료되거나 새로운 락을 얻을때까지.

-> 이 부분 솔직히 잘 모르겠다.

캐시 스탬피드에 대한 방법중에 locking 방법을 한번 다시 공부를 해야할듯 ?


그래서 논문에서는 Xfetch 라는 알고리즘을 확률적 알고리즘으로 소개한다.

##### 3. 확률적 조기 만료 (**Probabilistic early expiration**)

각각의 캐시가 만료되기 전에, 독립적인 확률적 결정으로 아이템을 re-generate 할 수 있다.
(근데 캐시가 만료되지 않았는데 왜 item 을 재생성하는가 ?
그러니까 조기 만료를 함으로써 refresh 하는건가 ? 그 이후에 consecutive 한 요청 중에
캐싱이 expiration 되어 cache stampede 가 일어나지 않게 하기 위해서 ?  
-> 그게 맞는듯 ...? )

조기 만료가 수행되는 확률은 request time 이 해당 아이템의 만료 시간에 근접함에 따라
증가한다.
(이건 또 뭔소리야 ? 읽다보면 이해가 되겠지)




```java
function Fetch(key, ttl; D)
	value, expiry ← CacheRead(key)
	gap ∼ D
	if !value or Time() + gap ≥ expiry then
		value ← RecomputeValue()
		CacheWrite(key, value, ttl)
	end
	return value
end
```

(캐쉬의 확률적 조기 만료 수도 코드.)


#### 조기만료 모델


##### Process Rate




##### Probabilistic early expirations




##### Effectiveness


###### Stampede size


###### Early expiration gap


###### Effectiveness



##### 균일 분포의 한계


##### 지수 분포의 최적성


##### 이론적 최적성 증명



##### XFetch의 간단한 수도 코드

```java
function XFetch(key, ttl; β = 1)
	value, ∆, expiry ← CacheRead(key)
	if !value or Time() − ∆β log(rand()) ≥ expiry then
		start ← Time()
		value ← RecomputeValue()
		∆ ← Time() – start
		CacheWrite(key, (value, ∆), ttl)
	end
	return value
end
```


XFetch는 구현이 매우 간단하고 매개변수 튜닝이 필요하지 않는다라...


이 내용을 이해하기 위해서는 과연
얼마나 많은 수학공부가 필요할지...