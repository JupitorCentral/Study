---
tags:
  - kafka
  - spring
  - event-sourcing
  - architecture
---

### [Kafka와 멀티모듈] Kafka는 Spring 앱 내부의 모듈 간, 혹은 마이크로서비스 간, 혹은 Spring과 DB 간의 로드 밸런싱에 사용되나요?

Kafka는 전통적인 의미의 로드 밸런싱(트래픽 분산) 도구가 아니지만, 분산 처리와 작업 부하 분산을 가능하게 합니다.

#### 마이크로서비스 간 (Between Microservices)

Kafka는 주로 마이크로서비스 간의 **비동기 통신**에 사용됩니다.
- 서비스 간의 결합도를 낮추는(Decoupling) 메시지 브로커 역할을 합니다.
- **컨슈머 그룹(Consumer Group)**을 통해 로드 밸런싱 효과를 냅니다. 같은 그룹 내의 여러 컨슈머 인스턴스가 토픽의 파티션(Partition)을 나눠서 처리함으로써 부하를 분산합니다.

#### Spring 애플리케이션 내부 (모듈 간)

단일 Spring 앱 내부의 모듈 간 통신에는 Kafka를 잘 쓰지 않습니다. 대신 **직접 메서드 호출**이나 Spring의 **ApplicationEventPublisher**를 사용하는 것이 일반적입니다. Kafka를 쓴다면 오버헤드가 너무 큽니다.

#### Spring ↔ Database

Kafka는 Spring과 DB 사이의 로드 밸런싱에 사용되지 않습니다. 하지만 **트랜잭션 일관성**을 맞추거나(KafkaTransactionManager), **Outbox 패턴**을 통해 DB 변경사항을 이벤트로 발행하는 데 사용됩니다.

---

### Spring 모듈끼리 Kafka 없이 통신 못 하나요? 왜 우리 프로젝트는 Kafka를 쓰죠? CQRS 때문인가요?

#### Kafka 없는 통신?

당연히 가능합니다. 직접 메서드를 호출하거나 HTTP 요청(REST), 혹은 RabbitMQ 같은 더 가벼운 큐를 쓸 수도 있습니다.

#### 뱅킹 프로젝트가 Kafka를 쓰는 이유

귀하의 프로젝트가 Kafka를 쓰는 핵심 이유는 **CQRS 구현**과 **뱅킹 시스템의 특수성** 때문일 가능성이 큽니다.

**1. CQRS 패턴 지원**
- **명령(Command)과 조회(Query) 분리:** 쓰기 모델과 읽기 모델이 다릅니다.
- **Event Sourcing:** Kafka에 모든 도메인 이벤트를 저장하여 '진실의 원천(Source of Truth)'으로 삼습니다.
- **이벤트 리플레이:** Kafka에 저장된 이벤트를 다시 읽어와서 언제든 읽기 모델(Read Model)을 재구축할 수 있습니다.

**2. 뱅킹 시스템의 요구사항**
- **감사 추적(Audit Trail):** 모든 거래 내역이 불변의 상태로 기록되어야 합니다.
- **결과적 일관성:** 계좌 잔액 등의 데이터가 여러 시스템 간에 시차를 두고 일치해지는 것을 관리합니다.
- **고성능:** 수천 건의 동시 거래를 처리할 수 있는 높은 처리량(Throughput)이 필요합니다.
- **내결함성(Fault Tolerance):** 금융 데이터의 무결성이 매우 중요합니다.

---

### 감사 추적(Audit Trail)은 그냥 트랜잭션 전에 DB를 스냅샷 찍어서 저장하면 안 되나요?

할 수는 있지만, 뱅킹 시스템에서 DB 스냅샷 방식은 Kafka 기반 이벤트 소싱에 비해 한계가 명확합니다.

#### Kafka 이벤트 소싱이 더 나은 이유

1.  **저장 공간 효율성:** 이벤트 소싱은 **변경된 부분(Delta)**만 저장합니다. 반면 DB 스냅샷은 변경되지 않은 데이터까지 포함한 전체 레코드를 복제하므로 공간 낭비가 심합니다.
2.  **시간적 쿼리 (Temporal Queries):** "오후 2시와 3시 사이에 무슨 일이 있었나?"를 파악하기 쉽습니다. 스냅샷은 "전"과 "후"만 보여줍니다.
3.  **비즈니스 문맥:** 이벤트는 `MoneyDeposited`(입금됨) 처럼 **"왜"** 변경되었는지를 담고 있습니다. DB 스냅샷은 그냥 잔액이 늘어난 결과만 보여줍니다.
4.  **불변성:** Kafka 로그는 기본적으로 수정 불가능(Immutable)하므로 감사 데이터로서의 신뢰도가 높습니다.

---

### 스토리지 효율성이 이해가 안 가요. 필요한 정보만 골라서 스냅샷 찍으면 효율적이지 않나요?

부분적으로 맞지만, 근본적으로 **무엇을 저장하느냐**의 차이입니다.

#### 비교: 델타(Delta) vs 상태(State)

- **이벤트 소싱 (Kafka):** 변경된 **차이값(Delta)**만 저장합니다.
    - 예: `계좌ID: 123, 변동액: +100원` (약 50바이트)
- **DB 스냅샷 (선별적 저장이라도):** 변경 후의 **결과 상태(State)**를 저장합니다.
    - 예: `계좌ID: 123, 현재잔액: 2500원, 예금주: 홍길동, ...` (약 200바이트 이상)

100만 건의 거래가 발생한다고 가정해 봅시다.
- **이벤트:** 100만 개의 아주 작은 변경 기록. (선형적 증가, 효율적)
- **스냅샷:** 100만 개의 상대적으로 큰 상태 기록. (급격한 증가)

또한 이벤트 소싱은 **재생(Replay)** 기능을 제공합니다. 작은 이벤트 조각들을 모아 언제든 과거의 상태를 만들어낼 수 있지만, 스냅샷 방식은 그 과정을 복원할 수 없습니다.

---

### DB 테이블 크기는 고정되어 있으니, 트랜잭션 데이터를 계속 저장하면 공간이 훨씬 많이 필요하다는 뜻인가요?

네, 맞습니다. 전통적인 DB 테이블 방식(각 트랜잭션을 행으로 저장)은 시간이 지날수록 Kafka 이벤트 로그보다 더 많은 공간을 차지하게 됩니다.

|방식|저장 내용|항목당 크기|공간 증가 속도|
|---|---|---|---|
|**이벤트 소싱 (Kafka)**|변경된 내역만 (Delta)|작음 (<100 bytes)|느림 (효율적)|
|**DB 트랜잭션 테이블**|트랜잭션 전체 정보|중간~큼|보통|
|**상태 스냅샷**|객체 전체 상태|가장 큼|매우 빠름|

결국 Kafka는 **"변경 사항"**만 순서대로 쌓기 때문에 장기적으로 볼 때 스토리지 효율과 성능 면에서 유리합니다.

---

### Kafka가 이벤트 소싱에 효율적인 이유를 구체적인 예시로 설명해 주세요.

1.  **Append-Only Log (순차 쓰기):**
    - 디스크의 헤드를 움직이지 않고 끝에만 데이터를 붙이기 때문에 쓰기 속도가 매우 빠릅니다.
    - 예: `입금`, `출금`, `이체`가 일어나는 순서대로 로그 끝에 추가만 함.
    
2.  **파티셔닝 (Partitioning):**
    - 데이터를 `accountId` 같은 키로 나누어 여러 파티션에 병렬로 저장합니다.
    - 예: 계좌 A의 거래는 파티션 1에, 계좌 B는 파티션 2에 저장하여 동시에 처리 가능.
    
3.  **로그 압축 (Log Compaction):**
    - 최신 상태만 남기고 과거 중복 데이터를 정리하는 기능입니다.
    - 예: 고객 주소가 10번 바뀌었을 때, `주소 변경` 이벤트 중 가장 최신의 1개만 남기고 나머지는 삭제하여 공간 절약.
    
4.  **재생 (Replay) 및 복구:**
    - 처음부터 이벤트를 다시 실행하여 현재 상태를 만듭니다.
    - 예: 조회용 DB가 날아가도, Kafka의 이벤트를 1번부터 다시 실행하면 완벽하게 복구됨.

---

### 그럼 제 프로젝트에서 Kafka는 이벤트 소싱 용도로만 쓰이나요?

아니요, 이벤트 소싱 외에도 다양하게 쓰이고 있을 겁니다.

- **이벤트 소싱 (주용도):** 모든 뱅킹 기록의 원본 저장소.
- **모듈 간 통신:** `계좌 서비스` -> `알림 서비스` 처럼 서로 직접 부르지 않고 이벤트를 통해 소통(Decoupling).
- **비동기 처리:** 이메일 발송, 분석 데이터 수집 등 시간이 걸리는 작업을 백그라운드에서 처리.
- **프로젝션(Read Model) 생성:** 이벤트를 스트리밍해서 조회 전용 DB(MongoDB 등)를 실시간으로 업데이트.

---

### "모듈 간 통신"이라고 했는데, 단일 Spring 서버라면 그냥 메서드 호출하면 되잖아요?

#### 단일 서버 (Monolith) vs 마이크로서비스

네, **단일 Spring 서버(Monolith)**라면 그냥 메서드 호출이 가장 빠르고 단순합니다. 굳이 Kafka를 쓸 필요가 없습니다.

#### Kafka가 필요한 경우

하지만 귀하의 프로젝트는 **마이크로서비스 아키텍처**를 지향하거나, 단일 서버라도 **내부 모듈 간의 결합도를 끊고 비동기로 처리**해야 하는 요구사항이 있기 때문에 Kafka를 쓰는 것입니다.

- **서비스가 분리될 때:** 서로 다른 프로세스(서버)에 있는 모듈끼리는 메서드 호출을 할 수 없습니다.
- **비동기가 필요할 때:** "가입 환영 메일 보내기" 같은 기능이 실패한다고 해서 "회원 가입" 자체가 실패하면 안 될 때, 이벤트를 던져놓고 가입 트랜잭션은 먼저 끝내기 위함입니다.

---

### 알림 같은 건 Kafka 없이 `@Async`로도 되잖아요?

네, 가능합니다. 간단한 비동기 작업은 Spring의 `@Async`나 `ApplicationEventPublisher`로 충분합니다.

#### Kafka를 쓰는 이유 (Overkill이 아닐 때)

- **신뢰성 (Durability):** 서버가 갑자기 꺼지면 `@Async`로 메모리에 있던 작업은 날아갑니다. Kafka는 디스크에 저장하므로 **서버가 재부팅되어도 작업을 잃어버리지 않고 이어서 처리**할 수 있습니다.
- **확장성 (Scalability):** 알림 트래픽이 폭주할 때, 알림 서버만 10대로 늘려서 Kafka에 쌓인 메시지를 빠르게 처리할 수 있습니다.
- **이력 보존:** 누가 언제 알림을 보냈는지 Kafka 로그에 다 남습니다.

단순한 프로젝트라면 Kafka는 과스펙이지만, **금융 시스템**처럼 데이터 유실이 없어야 하고 트래픽이 많은 곳에서는 Kafka가 필수적입니다.

---

### 프로젝션(Read Model)이 뭐죠? 이벤트를 스트리밍해서 뷰를 만든다는 게 이해가 안 가요.

#### 프로젝션 (Read Model)이란?

이벤트 소싱에서는 데이터가 "이벤트 조각"들로 흩어져 있어서 "현재 잔액 얼마야?" 같은 질문에 바로 답하기 어렵습니다. 그래서 **조회하기 좋게 미리 계산해서 만들어 둔 결과표**가 바로 프로젝션(Read Model)입니다.

#### 작동 원리 (예시)

1.  **이벤트 발생 (Kafka):**
    - `이벤트1: 계좌 생성 (잔액 0)`
    - `이벤트2: 1000원 입금`
    - `이벤트3: 500원 출금`
    
2.  **컨슈머 처리 (Projector):**
    - 이벤트를 하나씩 읽으면서 계산합니다. `0 + 1000 - 500 = 500`.
    
3.  **조회 DB 업데이트 (MongoDB/PostgreSQL):**
    - `계좌ID: A, 현재잔액: 500원` 이라고 DB에 저장합니다.
    
4.  **사용자 조회:**
    - 사용자가 "잔액 보여줘" 하면, 복잡한 계산 없이 저 DB만 딱 읽어서 보여줍니다. 빠릅니다.

이것이 바로 **쓰기(이벤트)**와 **읽기(프로젝션)**를 분리하는 **CQRS**의 핵심입니다.

---

### 요약 (Summary)

**Kafka의 핵심 역할**
- **마이크로서비스 통신:** 서비스 간 결합도를 낮추고 비동기 통신 지원.
- **부하 분산:** 컨슈머 그룹을 통해 병렬 처리 지원.

**이벤트 소싱과 Kafka**
- **불변 로그:** 뱅킹 시스템의 필수 요건인 '완벽한 감사 추적'을 위해 모든 변경 사항을 불변 이벤트로 저장.
- **저장 효율:** 전체 상태 스냅샷보다 변경분(Delta)만 저장하여 장기적으로 효율적.
- **복원력:** 언제든 이벤트를 처음부터 다시 실행(Replay)하여 시스템 상태 복구 가능.

**CQRS와 프로젝션**
- **쓰기(Command):** Kafka에 이벤트 저장.
- **읽기(Query):** 이벤트를 실시간으로 계산(Projection)하여 조회 전용 DB(Read Model)를 업데이트. 이를 통해 복잡한 조회를 매우 빠르게 처리.

**비동기 통신 선택 기준**
- **단일 서버:** `@Async`가 간단하고 좋음.
- **분산 시스템/금융:** 데이터 유실 방지와 고가용성을 위해 Kafka 사용 필수.